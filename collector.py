#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BioFoodLab ì‹ì•½ì²˜ ì¸í—ˆê°€ ë°ì´í„° ìˆ˜ì§‘ê¸°
- ë§¤ì¼ ìƒˆë²½ 3ì‹œ(KST) cron ì‹¤í–‰
- 16ê°œ APIì—ì„œ ì „êµ­ ë°ì´í„° ìˆ˜ì§‘ â†’ MariaDB ì €ì¥
- ì£¼ì†Œ íŒŒì‹±ìœ¼ë¡œ ì‹œë„/ì‹œêµ°êµ¬/ìë©´ë™ ì¸ë±ì‹±

ì‚¬ìš©ë²•:
  python3 collector.py              # ì¦ë¶„ ìˆ˜ì§‘ (ì–´ì œ ë³€ê²½ë¶„ë§Œ)
  python3 collector.py --full       # ì „ì²´ ìˆ˜ì§‘ (ìµœì´ˆ ì‹¤í–‰ ì‹œ)
  python3 collector.py --full I1220 # íŠ¹ì • APIë§Œ ì „ì²´ ìˆ˜ì§‘
  python3 collector.py --resume     # ì´ì–´ì„œ ìˆ˜ì§‘ (ì¤‘ë‹¨ëœ ì§€ì ë¶€í„°)
  python3 collector.py --auto       # ìë™ ëª¨ë“œ (ë¯¸ì™„ë£Œâ†’ì´ì–´ì„œ, ì™„ë£Œâ†’ì¦ë¶„) â˜… cronìš©
  python3 collector.py --status     # ìˆ˜ì§‘ í˜„í™© í™•ì¸
"""

import os
import sys
import json
import time
import re
import logging
import argparse
from datetime import datetime, timedelta
from urllib.request import urlopen, Request
from urllib.parse import quote
from urllib.error import URLError, HTTPError

import pymysql

# ============================================================
# ì„¤ì •
# ============================================================

DB_CONFIG = {
    'host': os.environ.get('FSS_DB_HOST', 'localhost'),
    'port': int(os.environ.get('FSS_DB_PORT', 3306)),
    'user': os.environ.get('FSS_DB_USER', 'fss_user'),
    'password': os.environ.get('FSS_DB_PASS', 'your_password_here'),
    'database': os.environ.get('FSS_DB_NAME', 'fss_data'),
    'charset': 'utf8mb4',
    'autocommit': False,
}

API_KEY = os.environ.get('FSS_API_KEY', 'e5a1d9f07d6c4424a757')
BASE_URL = 'https://openapi.foodsafetykorea.go.kr/api'
BATCH_SIZE = 100  # API 1íšŒ ìš”ì²­ë‹¹ ìµœëŒ€ ê±´ìˆ˜
REQUEST_DELAY = 0.3  # API ìš”ì²­ ê°„ê²©(ì´ˆ) - ì„œë²„ ë¶€í•˜ ë°©ì§€

# ============================================================
# API ì •ì˜
# ============================================================

# ì¹´í…Œê³ ë¦¬ë³„ API ëª©ë¡
BUSINESS_APIS = {
    'I1220':  {'name': 'ì‹í’ˆì œì¡°ê°€ê³µì—…',              'table': 'fss_businesses'},
    'I2829':  {'name': 'ì¦‰ì„íŒë§¤ì œì¡°ê°€ê³µì—…',          'table': 'fss_businesses'},
    'I-0020': {'name': 'ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì „ë¬¸/ë²¤ì²˜ì œì¡°ì—…', 'table': 'fss_businesses'},
    'I1300':  {'name': 'ì¶•ì‚°ë¬¼ ê°€ê³µì—…',               'table': 'fss_businesses'},
    'I1320':  {'name': 'ì¶•ì‚°ë¬¼ ì‹ìœ¡í¬ì¥ì²˜ë¦¬ì—…',       'table': 'fss_businesses'},
    'I2835':  {'name': 'ì‹ìœ¡ì¦‰ì„íŒë§¤ê°€ê³µì—…',          'table': 'fss_businesses'},
    'I2831':  {'name': 'ì‹í’ˆì†Œë¶„ì—…',                  'table': 'fss_businesses'},
    'C001':   {'name': 'ìˆ˜ì…ì‹í’ˆë“±ì˜ì—…ì‹ ê³ ',          'table': 'fss_businesses'},
    'I1260':  {'name': 'ì‹í’ˆë“±ìˆ˜ì…íŒë§¤ì—…',            'table': 'fss_businesses'},
}

PRODUCT_APIS = {
    'I1250':  {'name': 'ì‹í’ˆ(ì²¨ê°€ë¬¼)í’ˆëª©ì œì¡°ë³´ê³ ',    'table': 'fss_products'},
    'I1310':  {'name': 'ì¶•ì‚°ë¬¼ í’ˆëª©ì œì¡°ì •ë³´',         'table': 'fss_products'},
}

MATERIAL_APIS = {
    'C002':   {'name': 'ì‹í’ˆ(ì²¨ê°€ë¬¼)í’ˆëª©ì œì¡°ë³´ê³ (ì›ì¬ë£Œ)', 'table': 'fss_materials'},
    'C003':   {'name': 'ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ í’ˆëª©ì œì¡°ì‹ ê³ (ì›ì¬ë£Œ)', 'table': 'fss_materials'},
    'C006':   {'name': 'ì¶•ì‚°ë¬¼í’ˆëª©ì œì¡°ë³´ê³ (ì›ì¬ë£Œ)',       'table': 'fss_materials'},
}

CHANGE_APIS = {
    'I2859':  {'name': 'ì‹í’ˆì—…ì†Œ ì¸í—ˆê°€ ë³€ê²½ ì •ë³´',         'table': 'fss_changes'},
    'I2860':  {'name': 'ê±´ê°•ê¸°ëŠ¥ì‹í’ˆì—…ì†Œ ì¸í—ˆê°€ ë³€ê²½ ì •ë³´', 'table': 'fss_changes'},
}

ALL_APIS = {**BUSINESS_APIS, **PRODUCT_APIS, **MATERIAL_APIS, **CHANGE_APIS}

# ============================================================
# ë¡œê¹… ì„¤ì •
# ============================================================

LOG_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'logs')
os.makedirs(LOG_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(
            os.path.join(LOG_DIR, f'collect_{datetime.now().strftime("%Y%m%d")}.log'),
            encoding='utf-8'
        ),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================================
# ì£¼ì†Œ íŒŒì‹±
# ============================================================

def parse_address(addr):
    """ì£¼ì†Œ ë¬¸ìì—´ì—ì„œ ì‹œë„/ì‹œêµ°êµ¬/ìë©´ë™ ì¶”ì¶œ"""
    if not addr:
        return None, None, None

    addr = addr.strip()
    parts = addr.split()
    if not parts:
        return None, None, None

    sido = parts[0]  # "ê²½ê¸°ë„", "ì„œìš¸íŠ¹ë³„ì‹œ" ë“±
    sigungu = parts[1] if len(parts) > 1 else None
    dong = None

    if len(parts) > 2:
        # 3ë²ˆì§¸ ìš”ì†Œê°€ ì/ë©´/ë™/ë¦¬/ê°€ì¸ ê²½ìš°
        p2 = parts[2]
        if re.search(r'(ì|ë©´|ë™|ë¦¬|ê°€)$', p2):
            dong = p2
        # "OOë¡œ", "OOê¸¸" ë“± ë„ë¡œëª…ì€ ê±´ë„ˆëœ€

    return sido, sigungu, dong

# ============================================================
# API í˜¸ì¶œ
# ============================================================

# í•œë„ ì´ˆê³¼ ìƒíƒœ í”Œë˜ê·¸
_quota_exceeded = False

def fetch_api(service_id, start, end, chng_dt=None):
    """ì‹ì•½ì²˜ API í˜¸ì¶œ â†’ JSON íŒŒì‹±
    
    Returns:
        (total, rows, status)
        status: 'ok', 'empty', 'quota_exceeded', 'error'
    """
    global _quota_exceeded

    # ì´ë¯¸ í•œë„ ì´ˆê³¼ ìƒíƒœë©´ ìš”ì²­í•˜ì§€ ì•ŠìŒ
    if _quota_exceeded:
        return 0, [], 'quota_exceeded'

    url = f'{BASE_URL}/{API_KEY}/{service_id}/json/{start}/{end}'
    if chng_dt:
        url += f'/CHNG_DT={chng_dt}'

    try:
        req = Request(url, headers={'User-Agent': 'BioFoodLab-Collector/1.0'})
        with urlopen(req, timeout=30) as resp:
            raw = resp.read()

        # ì œì–´ë¬¸ì ì œê±° (ì¼ë¶€ API ì‘ë‹µì— í¬í•¨)
        text = raw.decode('utf-8', errors='replace')
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', text)

        data = json.loads(text)
        key = list(data.keys())[0]
        result = data[key]

        # ì˜¤ë¥˜ ì²´í¬
        if 'RESULT' in result:
            code = result['RESULT'].get('CODE', '')
            msg = result['RESULT'].get('MSG', '')

            if code == 'INFO-300':
                # í˜¸ì¶œ í•œë„ ì´ˆê³¼
                logger.warning(f'  âš ï¸ API í˜¸ì¶œ í•œë„ ì´ˆê³¼! ({msg})')
                _quota_exceeded = True
                return 0, [], 'quota_exceeded'

            if code == 'INFO-200':
                # í•´ë‹¹ ë°ì´í„° ì—†ìŒ
                return 0, [], 'empty'

            if code.startswith('ERROR'):
                logger.warning(f'  API ì˜¤ë¥˜ ({service_id}): [{code}] {msg}')
                return 0, [], 'error'

        # total_count ì•ˆì „ íŒŒì‹±
        tc = result.get('total_count', 0)
        total = int(tc) if tc != '' and tc is not None else 0

        rows = result.get('row', [])
        return total, rows, 'ok'

    except (json.JSONDecodeError, KeyError) as e:
        logger.warning(f'  JSON íŒŒì‹± ì˜¤ë¥˜ ({service_id} {start}-{end}): {e}')
        return 0, [], 'error'
    except (URLError, HTTPError, TimeoutError) as e:
        logger.warning(f'  ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ({service_id} {start}-{end}): {e}')
        return 0, [], 'error'
    except Exception as e:
        logger.error(f'  ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ({service_id} {start}-{end}): {e}')
        return 0, [], 'error'

# ============================================================
# DB ì €ì¥
# ============================================================

def upsert_business(cursor, api_source, row):
    """ì—…ì†Œ ì¸í—ˆê°€ UPSERT"""
    addr = row.get('LOCP_ADDR', '')
    sido, sigungu, dong = parse_address(addr)

    sql = """
    INSERT INTO fss_businesses
        (api_source, lcns_no, bssh_nm, prsdnt_nm, induty_nm, prms_dt,
         telno, locp_addr, instt_nm, clsbiz_dvs_nm,
         addr_sido, addr_sigungu, addr_dong)
    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
    ON DUPLICATE KEY UPDATE
        bssh_nm = VALUES(bssh_nm),
        prsdnt_nm = VALUES(prsdnt_nm),
        induty_nm = VALUES(induty_nm),
        prms_dt = VALUES(prms_dt),
        telno = VALUES(telno),
        locp_addr = VALUES(locp_addr),
        instt_nm = VALUES(instt_nm),
        clsbiz_dvs_nm = VALUES(clsbiz_dvs_nm),
        addr_sido = VALUES(addr_sido),
        addr_sigungu = VALUES(addr_sigungu),
        addr_dong = VALUES(addr_dong)
    """
    cursor.execute(sql, (
        api_source,
        row.get('LCNS_NO', ''),
        row.get('BSSH_NM', ''),
        row.get('PRSDNT_NM'),
        row.get('INDUTY_NM'),
        row.get('PRMS_DT'),
        row.get('TELNO'),
        addr,
        row.get('INSTT_NM'),
        row.get('CLSBIZ_DVS_NM'),
        sido, sigungu, dong
    ))
    return cursor.rowcount  # 1=INSERT, 2=UPDATE


def upsert_product(cursor, api_source, row):
    """í’ˆëª© ì œì¡° ë³´ê³  UPSERT"""
    report_no = row.get('PRDLST_REPORT_NO', '')
    if not report_no:
        return 0

    sql = """
    INSERT INTO fss_products
        (api_source, lcns_no, bssh_nm, prdlst_report_no, prms_dt,
         prdlst_nm, prdlst_dcnm, production, hieng_lntrt_dvs_nm,
         child_crtfc_yn, pog_daycnt, induty_cd_nm, dispos, shap,
         stdr_stnd, ntk_mthd, primary_fnclty, iftkn_atnt_matr_cn,
         cstdy_mthd, prdt_shap_cd_nm, usage_info, prpos,
         frmlc_mtrqlt, qlity_mntnc, etqty_xport, last_updt_dtm)
    VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
    ON DUPLICATE KEY UPDATE
        lcns_no = VALUES(lcns_no),
        bssh_nm = VALUES(bssh_nm),
        prms_dt = VALUES(prms_dt),
        prdlst_nm = VALUES(prdlst_nm),
        prdlst_dcnm = VALUES(prdlst_dcnm),
        production = VALUES(production),
        pog_daycnt = VALUES(pog_daycnt),
        last_updt_dtm = VALUES(last_updt_dtm)
    """
    cursor.execute(sql, (
        api_source,
        row.get('LCNS_NO', ''),
        row.get('BSSH_NM'),
        report_no,
        row.get('PRMS_DT'),
        row.get('PRDLST_NM'),
        row.get('PRDLST_DCNM'),
        row.get('PRODUCTION'),
        row.get('HIENG_LNTRT_DVS_NM'),
        row.get('CHILD_CRTFC_YN'),
        row.get('POG_DAYCNT'),
        row.get('INDUTY_CD_NM'),
        row.get('DISPOS'),
        row.get('SHAP'),
        row.get('STDR_STND'),
        row.get('NTK_MTHD'),
        row.get('PRIMARY_FNCLTY'),
        row.get('IFTKN_ATNT_MATR_CN'),
        row.get('CSTDY_MTHD'),
        row.get('PRDT_SHAP_CD_NM'),
        row.get('USAGE'),
        row.get('PRPOS'),
        row.get('FRMLC_MTRQLT'),
        row.get('QLITY_MNTNC_TMLMT_DAYCNT'),
        row.get('ETQTY_XPORT_PRDLST_YN'),
        row.get('LAST_UPDT_DTM'),
    ))
    return cursor.rowcount


def upsert_material(cursor, api_source, row):
    """ì›ì¬ë£Œ ì •ë³´ UPSERT"""
    report_no = row.get('PRDLST_REPORT_NO', '')
    ordno = row.get('RAWMTRL_ORDNO', '0')
    if not report_no:
        return 0

    sql = """
    INSERT INTO fss_materials
        (api_source, lcns_no, bssh_nm, prdlst_report_no, prms_dt,
         prdlst_nm, prdlst_dcnm, rawmtrl_nm, rawmtrl_ordno,
         chng_dt, etqty_xport)
    VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
    ON DUPLICATE KEY UPDATE
        lcns_no = VALUES(lcns_no),
        bssh_nm = VALUES(bssh_nm),
        prdlst_nm = VALUES(prdlst_nm),
        rawmtrl_nm = VALUES(rawmtrl_nm),
        chng_dt = VALUES(chng_dt)
    """
    cursor.execute(sql, (
        api_source,
        row.get('LCNS_NO'),
        row.get('BSSH_NM'),
        report_no,
        row.get('PRMS_DT'),
        row.get('PRDLST_NM'),
        row.get('PRDLST_DCNM'),
        row.get('RAWMTRL_NM'),
        ordno,
        row.get('CHNG_DT'),
        row.get('ETQTY_XPORT_PRDLST_YN'),
    ))
    return cursor.rowcount


def insert_change(cursor, api_source, row):
    """ë³€ê²½ ì´ë ¥ INSERT (ì¤‘ë³µ í—ˆìš©)"""
    sql = """
    INSERT INTO fss_changes
        (api_source, lcns_no, bssh_nm, induty_cd_nm, telno,
         site_addr, chng_dt, chng_bf_cn, chng_af_cn, chng_prvns)
    VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
    """
    cursor.execute(sql, (
        api_source,
        row.get('LCNS_NO', ''),
        row.get('BSSH_NM'),
        row.get('INDUTY_CD_NM'),
        row.get('TELNO'),
        row.get('SITE_ADDR'),
        row.get('CHNG_DT'),
        row.get('CHNG_BF_CN'),
        row.get('CHNG_AF_CN'),
        row.get('CHNG_PRVNS'),
    ))
    return 1

# ============================================================
# DBì—ì„œ ê¸°ì¡´ ìˆ˜ì§‘ ê±´ìˆ˜ í™•ì¸
# ============================================================

def get_collected_count(conn, service_id, table):
    """í•´ë‹¹ APIì˜ DB ì €ì¥ ê±´ìˆ˜ ì¡°íšŒ"""
    try:
        cursor = conn.cursor()
        cursor.execute(f"SELECT COUNT(*) FROM {table} WHERE api_source=%s", (service_id,))
        count = cursor.fetchone()[0]
        cursor.close()
        return count
    except Exception:
        return 0

# ============================================================
# ìˆ˜ì§‘ ë©”ì¸ ë¡œì§
# ============================================================

def collect_api(conn, service_id, api_info, collect_type='incremental',
                chng_dt=None, resume=False):
    """ë‹¨ì¼ API ì „ì²´ ìˆ˜ì§‘
    
    resume=Trueì´ë©´ DBì— ì´ë¯¸ ì €ì¥ëœ ê±´ìˆ˜ ì´í›„ë¶€í„° ìˆ˜ì§‘
    """
    global _quota_exceeded

    table = api_info['table']
    name = api_info['name']

    # ì´ë¯¸ í•œë„ ì´ˆê³¼ ìƒíƒœë©´ ê±´ë„ˆëœ€
    if _quota_exceeded:
        logger.info(f'â­ï¸ [{service_id}] {name} â€” í•œë„ ì´ˆê³¼ë¡œ ê±´ë„ˆëœ€')
        return 'quota_exceeded'

    started_at = datetime.now()
    logger.info(f'â–¶ [{service_id}] {name} ìˆ˜ì§‘ ì‹œì‘ ({collect_type})')

    # 1ë‹¨ê³„: ì´ ê±´ìˆ˜ í™•ì¸
    dt_param = chng_dt if collect_type == 'incremental' and chng_dt else None
    total, _, status = fetch_api(service_id, 1, 1, dt_param)

    if status == 'quota_exceeded':
        logger.warning(f'  â­ï¸ í•œë„ ì´ˆê³¼ â€” ì´ API ê±´ë„ˆëœ€')
        log_collection(conn, service_id, name, collect_type, 0, 0, 0, 0, 0,
                       'API í˜¸ì¶œ í•œë„ ì´ˆê³¼ (INFO-300)', started_at)
        return 'quota_exceeded'

    if total == 0:
        logger.info(f'  â†’ ë°ì´í„° ì—†ìŒ (0ê±´)')
        log_collection(conn, service_id, name, collect_type, 0, 0, 0, 0, 0, None, started_at)
        return 'empty'

    # resume ëª¨ë“œ: ì´ë¯¸ ìˆ˜ì§‘ëœ ê±´ìˆ˜ í™•ì¸ í›„ ì‹œì‘ ì§€ì  ê²°ì •
    start_from = 1
    if resume:
        already = get_collected_count(conn, service_id, table)
        if already >= total:
            logger.info(f'  âœ… ì´ë¯¸ ì™„ë£Œ: DB {already:,}ê±´ / API {total:,}ê±´')
            log_collection(conn, service_id, name, collect_type + '_resume',
                           total, 0, 0, 0, 0, 'ì´ë¯¸ ìˆ˜ì§‘ ì™„ë£Œ', started_at)
            return 'already_done'
        elif already > 0:
            start_from = already + 1
            logger.info(f'  ğŸ”„ ì´ì–´ì„œ ìˆ˜ì§‘: DB {already:,}ê±´ ë³´ìœ  â†’ {start_from:,}ë²ˆë¶€í„° ì‹œì‘ (ì´ {total:,}ê±´)')
        else:
            logger.info(f'  ì´ {total:,}ê±´ ìˆ˜ì§‘ ì˜ˆì •')
    else:
        logger.info(f'  ì´ {total:,}ê±´ ìˆ˜ì§‘ ì˜ˆì •')

    # 2ë‹¨ê³„: í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ìˆ˜ì§‘
    fetched = 0
    inserted = 0
    updated = 0
    errors = 0
    error_msg = None

    cursor = conn.cursor()

    try:
        for start in range(start_from, total + 1, BATCH_SIZE):
            end = min(start + BATCH_SIZE - 1, total)
            _, rows, status = fetch_api(service_id, start, end, dt_param)

            # í•œë„ ì´ˆê³¼ ê°ì§€ â†’ ì—¬ê¸°ê¹Œì§€ ì €ì¥í•˜ê³  ì¤‘ë‹¨
            if status == 'quota_exceeded':
                logger.warning(f'  âš ï¸ {fetched:,}ê±´ê¹Œì§€ ìˆ˜ì§‘ í›„ í•œë„ ì´ˆê³¼ë¡œ ì¤‘ë‹¨')
                conn.commit()
                error_msg = f'API í•œë„ ì´ˆê³¼ â€” {start_from}~{start-1}ë²ˆ ìˆ˜ì§‘ ì™„ë£Œ'
                break

            if not rows:
                time.sleep(REQUEST_DELAY)
                continue

            for row in rows:
                try:
                    if table == 'fss_businesses':
                        rc = upsert_business(cursor, service_id, row)
                    elif table == 'fss_products':
                        rc = upsert_product(cursor, service_id, row)
                    elif table == 'fss_materials':
                        rc = upsert_material(cursor, service_id, row)
                    elif table == 'fss_changes':
                        rc = insert_change(cursor, service_id, row)
                    else:
                        continue

                    if rc == 1:
                        inserted += 1
                    elif rc == 2:
                        updated += 1
                    fetched += 1

                except pymysql.Error as e:
                    errors += 1
                    if errors <= 5:
                        logger.warning(f'  DB ì˜¤ë¥˜: {e} / row={row.get("LCNS_NO", "?")}')
                    if not error_msg:
                        error_msg = str(e)

            # ë°°ì¹˜ ì»¤ë°‹ (100ê±´ë§ˆë‹¤)
            conn.commit()

            # ì§„í–‰ë¥  ë¡œê·¸ (1000ê±´ë§ˆë‹¤)
            total_fetched = (start_from - 1) + fetched if resume else fetched
            if fetched > 0 and fetched % 1000 < BATCH_SIZE:
                pct = (total_fetched / total * 100) if total else 0
                logger.info(f'  ì§„í–‰: {total_fetched:,}/{total:,} ({pct:.1f}%) ì…ë ¥={inserted} ê°±ì‹ ={updated}')

            time.sleep(REQUEST_DELAY)

        conn.commit()

    except Exception as e:
        logger.error(f'  ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}')
        error_msg = str(e)
        conn.rollback()
    finally:
        cursor.close()

    elapsed = (datetime.now() - started_at).total_seconds()
    result_status = 'quota_exceeded' if _quota_exceeded else 'ok'

    if result_status == 'ok':
        logger.info(f'  âœ… ì™„ë£Œ: {fetched:,}ê±´ (ì…ë ¥={inserted}, ê°±ì‹ ={updated}, ì˜¤ë¥˜={errors}) [{elapsed:.0f}ì´ˆ]')
    else:
        logger.info(f'  â¸ï¸ ì¤‘ë‹¨: {fetched:,}ê±´ ì €ì¥ë¨ (ì…ë ¥={inserted}, ê°±ì‹ ={updated}) [{elapsed:.0f}ì´ˆ]')

    log_collection(conn, service_id, name,
                   collect_type + ('_resume' if resume else ''),
                   total, fetched, inserted, updated, errors, error_msg, started_at)

    return result_status


def collect_api_auto(conn, service_id, api_info):
    """ìë™ ëª¨ë“œ: ë¯¸ì™„ë£Œ APIëŠ” ì´ì–´ì„œ ìˆ˜ì§‘, ì™„ë£Œëœ APIëŠ” ì¦ë¶„ ìˆ˜ì§‘
    
    1) API ì´ ê±´ìˆ˜ í™•ì¸
    2) DB ê±´ìˆ˜ì™€ ë¹„êµ
    3) DB < API â†’ resume (ì´ì–´ì„œ ìˆ˜ì§‘)
    4) DB >= API â†’ incremental (ì–´ì œ ë³€ê²½ë¶„ë§Œ)
    """
    global _quota_exceeded

    if _quota_exceeded:
        logger.info(f'â­ï¸ [{service_id}] {api_info["name"]} â€” í•œë„ ì´ˆê³¼ë¡œ ê±´ë„ˆëœ€')
        return 'quota_exceeded'

    table = api_info['table']
    name = api_info['name']

    # 1) API ì´ ê±´ìˆ˜ í™•ì¸ (ì „ì²´ ëª¨ë“œ)
    api_total, _, status = fetch_api(service_id, 1, 1, None)

    if status == 'quota_exceeded':
        return 'quota_exceeded'

    # 2) DB ê±´ìˆ˜ í™•ì¸
    db_count = get_collected_count(conn, service_id, table)

    # 3) íŒë‹¨: ë¯¸ì™„ë£Œ vs ì™„ë£Œ
    if api_total > 0 and db_count < api_total:
        # ë¯¸ì™„ë£Œ â†’ resume ëª¨ë“œ (ì „ì²´ ìˆ˜ì§‘ ì´ì–´ì„œ)
        pct = (db_count / api_total * 100) if api_total else 0
        logger.info(f'ğŸ“Š [{service_id}] {name}: DB {db_count:,} / API {api_total:,} ({pct:.1f}%) â†’ ì´ì–´ì„œ ìˆ˜ì§‘')
        return collect_api(conn, service_id, api_info,
                          collect_type='full', chng_dt=None, resume=True)
    else:
        # ì™„ë£Œ â†’ ì¦ë¶„ ìˆ˜ì§‘ (ì–´ì œ ë³€ê²½ë¶„ë§Œ)
        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
        logger.info(f'âœ… [{service_id}] {name}: DB {db_count:,} / API {api_total:,} â€” ì¦ë¶„ ìˆ˜ì§‘ ({yesterday})')
        return collect_api(conn, service_id, api_info,
                          collect_type='incremental', chng_dt=yesterday, resume=False)


def log_collection(conn, api_source, api_name, collect_type,
                   total, fetched, inserted, updated, errors, error_msg, started_at):
    """ìˆ˜ì§‘ ë¡œê·¸ ê¸°ë¡"""
    finished_at = datetime.now()
    duration = int((finished_at - started_at).total_seconds())
    try:
        cursor = conn.cursor()
        cursor.execute("""
            INSERT INTO fss_collect_log
                (api_source, api_name, collect_type, total_count, fetched_count,
                 inserted_count, updated_count, error_count, error_msg,
                 started_at, finished_at, duration_sec)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (api_source, api_name, collect_type, total, fetched,
              inserted, updated, errors, error_msg,
              started_at, finished_at, duration))
        conn.commit()
        cursor.close()
    except Exception as e:
        logger.error(f'ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨: {e}')

# ============================================================
# ìˆ˜ì§‘ í˜„í™©
# ============================================================

def show_status(conn):
    """í˜„ì¬ DB ìˆ˜ì§‘ í˜„í™© ì¶œë ¥"""
    cursor = conn.cursor(pymysql.cursors.DictCursor)

    print('\n' + '='*70)
    print('  ì‹ì•½ì²˜ ë°ì´í„° ìˆ˜ì§‘ í˜„í™©')
    print('='*70)

    # í…Œì´ë¸”ë³„ ê±´ìˆ˜
    tables = [
        ('fss_businesses', 'ì—…ì†Œ ì¸í—ˆê°€'),
        ('fss_products', 'í’ˆëª© ì œì¡°'),
        ('fss_materials', 'ì›ì¬ë£Œ'),
        ('fss_changes', 'ë³€ê²½ ì´ë ¥'),
    ]
    print(f'\n{"í…Œì´ë¸”":<20} {"ê±´ìˆ˜":>12} {"ìµœì‹ ìˆ˜ì§‘":>20}')
    print('-'*55)
    for tbl, name in tables:
        try:
            cursor.execute(f'SELECT COUNT(*) as cnt, MAX(collected_at) as last_dt FROM {tbl}')
            row = cursor.fetchone()
            cnt = f"{row['cnt']:,}" if row['cnt'] else '0'
            last = str(row['last_dt'])[:16] if row['last_dt'] else '-'
            print(f'{name:<20} {cnt:>12} {last:>20}')
        except:
            print(f'{name:<20} {"(í…Œì´ë¸”ì—†ìŒ)":>12}')

    # APIë³„ ê±´ìˆ˜
    print(f'\n{"API":<10} {"ì´ë¦„":<25} {"DBê±´ìˆ˜":>10} {"ìƒíƒœ":>8}')
    print('-'*60)
    for sid, info in ALL_APIS.items():
        try:
            cursor.execute(f"SELECT COUNT(*) as cnt FROM {info['table']} WHERE api_source=%s", (sid,))
            row = cursor.fetchone()
            cnt = row['cnt']
            cnt_str = f"{cnt:,}"
            status = 'âœ…' if cnt > 0 else 'â¬œ'
        except:
            cnt_str = '-'
            status = 'âŒ'
        print(f'{sid:<10} {info["name"]:<25} {cnt_str:>10} {status:>8}')

    # ìµœê·¼ ìˆ˜ì§‘ ë¡œê·¸
    print(f'\nìµœê·¼ ìˆ˜ì§‘ ì´ë ¥:')
    print('-'*70)
    try:
        cursor.execute("""
            SELECT api_source, api_name, collect_type, fetched_count,
                   inserted_count, updated_count, error_count, error_msg,
                   started_at, duration_sec
            FROM fss_collect_log
            ORDER BY started_at DESC LIMIT 10
        """)
        for r in cursor.fetchall():
            dt = str(r['started_at'])[:16]
            err_info = f" âš ï¸{r['error_msg'][:30]}" if r['error_msg'] else ''
            print(f"  {dt} [{r['api_source']}] {r['api_name']} "
                  f"({r['collect_type']}) {r['fetched_count']:,}ê±´ "
                  f"(+{r['inserted_count']}/â†»{r['updated_count']}/âœ—{r['error_count']})"
                  f" {r['duration_sec']}ì´ˆ{err_info}")
    except:
        print('  (ë¡œê·¸ ì—†ìŒ)')

    print()
    cursor.close()

# ============================================================
# ë©”ì¸
# ============================================================

def load_schedule_config(conn):
    """DBì—ì„œ ìŠ¤ì¼€ì¤„ ì„¤ì • ì½ê¸°"""
    try:
        cursor = conn.cursor(pymysql.cursors.DictCursor)
        cursor.execute('SELECT * FROM fss_schedule_config WHERE id=1')
        row = cursor.fetchone()
        cursor.close()
        return row
    except Exception:
        return None


def load_api_configs(conn):
    """DBì—ì„œ API ì„¤ì • ì½ê¸°"""
    try:
        cursor = conn.cursor(pymysql.cursors.DictCursor)
        cursor.execute('SELECT api_source, is_enabled, enabled_fields FROM fss_api_config')
        rows = cursor.fetchall()
        cursor.close()
        return rows if rows else None
    except Exception:
        return None


def main():
    global _quota_exceeded

    parser = argparse.ArgumentParser(description='ì‹ì•½ì²˜ ì¸í—ˆê°€ ë°ì´í„° ìˆ˜ì§‘ê¸°')
    parser.add_argument('--full', action='store_true', help='ì „ì²´ ìˆ˜ì§‘ (ìµœì´ˆ ì‹¤í–‰)')
    parser.add_argument('--resume', action='store_true', help='ì´ì–´ì„œ ìˆ˜ì§‘ (ì¤‘ë‹¨ëœ ì§€ì ë¶€í„°)')
    parser.add_argument('--auto', action='store_true', help='ìë™ ëª¨ë“œ (ë¯¸ì™„ë£Œâ†’ì´ì–´ì„œ, ì™„ë£Œâ†’ì¦ë¶„) cronìš©')
    parser.add_argument('--status', action='store_true', help='ìˆ˜ì§‘ í˜„í™© í™•ì¸')
    parser.add_argument('apis', nargs='*', help='íŠ¹ì • APIë§Œ ìˆ˜ì§‘ (ì˜ˆ: I1220 I2829)')
    args = parser.parse_args()

    # DB ì—°ê²°
    try:
        conn = pymysql.connect(**DB_CONFIG)
        logger.info(f'DB ì—°ê²° ì„±ê³µ: {DB_CONFIG["host"]}:{DB_CONFIG["port"]}/{DB_CONFIG["database"]}')
    except pymysql.Error as e:
        logger.error(f'DB ì—°ê²° ì‹¤íŒ¨: {e}')
        sys.exit(1)

    try:
        if args.status:
            show_status(conn)
            return

        # â”€â”€â”€ DBì—ì„œ ì„¤ì • ì½ê¸° â”€â”€â”€
        db_schedule = load_schedule_config(conn)
        db_api_configs = load_api_configs(conn)

        # DB ì„¤ì •ì´ ìˆìœ¼ë©´ ì ìš©
        global API_KEY, BATCH_SIZE, REQUEST_DELAY
        if db_schedule:
            if db_schedule.get('api_key'):
                API_KEY = db_schedule['api_key']
            if db_schedule.get('batch_size'):
                BATCH_SIZE = int(db_schedule['batch_size'])
            if db_schedule.get('request_delay'):
                REQUEST_DELAY = float(db_schedule['request_delay'])
            logger.info(f'DB ì„¤ì • ë¡œë“œ: batch={BATCH_SIZE}, delay={REQUEST_DELAY}s')

        # resumeì´ë©´ í•­ìƒ full ëª¨ë“œ
        if args.resume:
            collect_type = 'full'
        elif args.auto:
            collect_type = 'auto'
        else:
            collect_type = 'full' if args.full else (
                db_schedule.get('collect_mode', 'incremental') if db_schedule else 'incremental'
            )

        # ì¦ë¶„ ìˆ˜ì§‘: ì–´ì œ ë‚ ì§œ ê¸°ì¤€
        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
        chng_dt = yesterday if collect_type not in ('full', 'auto') else None

        # ìˆ˜ì§‘ ëŒ€ìƒ ê²°ì •
        if args.apis:
            # ëª…ë ¹ì¤„ ì¸ì ìš°ì„ 
            targets = {k: v for k, v in ALL_APIS.items() if k in args.apis}
            if not targets:
                logger.error(f'ìœ íš¨í•˜ì§€ ì•Šì€ API: {args.apis}')
                logger.info(f'ì‚¬ìš© ê°€ëŠ¥í•œ API: {list(ALL_APIS.keys())}')
                return
        elif db_api_configs:
            # DB ì„¤ì •ì—ì„œ í™œì„±í™”ëœ APIë§Œ
            enabled_ids = [c['api_source'] for c in db_api_configs if c.get('is_enabled')]
            targets = {k: v for k, v in ALL_APIS.items() if k in enabled_ids}
            if not targets:
                logger.warning('DB ì„¤ì •ì— í™œì„±í™”ëœ APIê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ APIë¡œ ì§„í–‰í•©ë‹ˆë‹¤.')
                targets = ALL_APIS
            else:
                logger.info(f'DB ì„¤ì • ê¸°ì¤€: {len(targets)}ê°œ API í™œì„±')
        else:
            targets = ALL_APIS

        logger.info(f'{"="*60}')
        logger.info(f'ì‹ì•½ì²˜ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ({collect_type}{"+ resume" if args.resume else ""})')
        logger.info(f'ëŒ€ìƒ: {len(targets)}ê°œ API')
        logger.info(f'ì„¤ì •: batch_size={BATCH_SIZE}, delay={REQUEST_DELAY}s')
        if chng_dt:
            logger.info(f'ë³€ê²½ì¼ì ê¸°ì¤€: {chng_dt} ì´í›„')
        logger.info(f'{"="*60}')

        total_start = time.time()
        _quota_exceeded = False  # ì´ˆê¸°í™”

        # ìˆ˜ì§‘ ìˆœì„œ: ì—…ì†Œ â†’ í’ˆëª© â†’ ì›ì¬ë£Œ â†’ ë³€ê²½ì´ë ¥
        order = list(BUSINESS_APIS.keys()) + list(PRODUCT_APIS.keys()) + \
                list(MATERIAL_APIS.keys()) + list(CHANGE_APIS.keys())

        completed = []
        skipped = []

        for sid in order:
            if sid in targets:
                if collect_type == 'auto':
                    # ìë™ ëª¨ë“œ: APIë³„ë¡œ ë¯¸ì™„ë£Œ/ì™„ë£Œ íŒë‹¨í•˜ì—¬ ìˆ˜ì§‘
                    result = collect_api_auto(conn, sid, targets[sid])
                else:
                    result = collect_api(conn, sid, targets[sid], collect_type,
                                         chng_dt, resume=args.resume)
                if result == 'quota_exceeded':
                    skipped.append(sid)
                else:
                    completed.append(sid)

        elapsed = time.time() - total_start
        logger.info(f'{"="*60}')
        logger.info(f'ìˆ˜ì§‘ ì¢…ë£Œ! ì´ ì†Œìš”ì‹œê°„: {elapsed:.0f}ì´ˆ ({elapsed/60:.1f}ë¶„)')
        logger.info(f'  ì™„ë£Œ: {len(completed)}ê°œ API {completed}')
        if skipped:
            logger.info(f'  â­ï¸ í•œë„ì´ˆê³¼ ê±´ë„ˆëœ€: {len(skipped)}ê°œ API {skipped}')
            logger.info(f'  â†’ ë‚´ì¼ ìë™ ìˆ˜ì§‘ ì‹œ ì´ì–´ì„œ ì§„í–‰ë©ë‹ˆë‹¤ (--auto)')
        logger.info(f'{"="*60}')

        show_status(conn)

    finally:
        conn.close()


if __name__ == '__main__':
    main()
